{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f906fc-1bb1-4062-87da-db1259ff57c4",
   "metadata": {
    "id": "f9f906fc-1bb1-4062-87da-db1259ff57c4"
   },
   "outputs": [],
   "source": [
    "# Block 1: Imports\n",
    "import os\n",
    "os.environ.setdefault(\"DDE_BACKEND\", \"pytorch\")\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import MAS_library as MASL\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92bfd4e0-3b45-45c3-8756-d801cee44f18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92bfd4e0-3b45-45c3-8756-d801cee44f18",
    "outputId": "751bec4d-27ac-4eeb-c103-e7f84e69b112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run] RUN_NAME=T2ROI-emph V2 Compare\n",
      "[run] RUN_DIR=./runs/T2ROI-emph V2 Compare\n",
      "[run] CKPT_DIR=./runs/T2ROI-emph V2 Compare/checkpoints\n",
      "[run] LOGS_DIR=./runs/T2ROI-emph V2 Compare/logs\n",
      "[run] checkpoint_path=./runs/T2ROI-emph V2 Compare/checkpoints/T2ROI-emph V2 Compare_checkpoint.pt\n",
      "[run] best_model_path=./runs/T2ROI-emph V2 Compare/checkpoints/T2ROI-emph V2 Compare_best_model.pt\n",
      "[run] metrics_log_path=./runs/T2ROI-emph V2 Compare/logs/T2ROI-emph V2 Compare_epoch_metrics.txt\n",
      "[run] train_log_path=./runs/T2ROI-emph V2 Compare/logs/T2ROI-emph V2 Compare_train_log.txt\n",
      "[run] diag_jsonl_path=./runs/T2ROI-emph V2 Compare/logs/T2ROI-emph V2 Compare_epoch_diag.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Run/Paths/IO\n",
    "RUN_NAME = \"T2ROI-emph V2 Compare\"\n",
    "RESUME_FROM_CHECKPOINT = False\n",
    "RESET_LOGS_ON_FRESH_START = True\n",
    "\n",
    "BASE_DIR    = \"./runs\"\n",
    "RUN_DIR     = os.path.join(BASE_DIR, RUN_NAME)\n",
    "CKPT_DIR    = os.path.join(RUN_DIR, \"checkpoints\")\n",
    "LOGS_DIR    = os.path.join(RUN_DIR, \"logs\")\n",
    "DATA_DIR    = \"./data\"\n",
    "PRECOMP_DIR = os.path.join(DATA_DIR, \"precomputed\")\n",
    "\n",
    "checkpoint_path   = os.path.join(CKPT_DIR, f\"{RUN_NAME}_checkpoint.pt\")\n",
    "best_model_path   = os.path.join(CKPT_DIR, f\"{RUN_NAME}_best_model.pt\")\n",
    "metrics_log_path  = os.path.join(LOGS_DIR,  f\"{RUN_NAME}_epoch_metrics.txt\")\n",
    "train_log_path    = os.path.join(LOGS_DIR,  f\"{RUN_NAME}_train_log.txt\")\n",
    "diag_jsonl_path   = os.path.join(LOGS_DIR,  f\"{RUN_NAME}_epoch_diag.jsonl\")\n",
    "\n",
    "for d in (BASE_DIR, RUN_DIR, CKPT_DIR, LOGS_DIR, DATA_DIR, PRECOMP_DIR):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def _append_train_line(text: str):\n",
    "    try:\n",
    "        with open(train_log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(text.rstrip() + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] couldn't write train log: {e}\")\n",
    "\n",
    "def _append_jsonl_line(text: str):\n",
    "    try:\n",
    "        with open(diag_jsonl_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(text.rstrip() + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] couldn't write diag jsonl: {e}\")\n",
    "\n",
    "print(f\"[run] RUN_NAME={RUN_NAME}\")\n",
    "print(f\"[run] RUN_DIR={RUN_DIR}\")\n",
    "print(f\"[run] CKPT_DIR={CKPT_DIR}\")\n",
    "print(f\"[run] LOGS_DIR={LOGS_DIR}\")\n",
    "print(f\"[run] checkpoint_path={checkpoint_path}\")\n",
    "print(f\"[run] best_model_path={best_model_path}\")\n",
    "print(f\"[run] metrics_log_path={metrics_log_path}\")\n",
    "print(f\"[run] train_log_path={train_log_path}\")\n",
    "print(f\"[run] diag_jsonl_path={diag_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b2ee3f-64ac-4b50-95b6-cc1421580d98",
   "metadata": {
    "id": "75b2ee3f-64ac-4b50-95b6-cc1421580d98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu128\n",
      "cuda runtime in torch: 12.8\n",
      "CUDA available: True\n",
      "GPU[0]: NVIDIA A100-SXM4-80GB\n",
      "device: cuda\n",
      "[units] DISP_NORM=box  DISP_SCALE=0.01  DISP_INV_SCALE=100\n"
     ]
    }
   ],
   "source": [
    "# Block 3: Device/Seeds/Knobs\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda runtime in torch:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU[0]:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", device)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "BoxSize  = 100.0\n",
    "grid     = 64\n",
    "cellsize = BoxSize / grid\n",
    "MAS      = 'CIC'\n",
    "verbose  = False\n",
    "threads  = 4\n",
    "\n",
    "num_sim            = 1000\n",
    "lambda_p           = 1.0\n",
    "total_epochs       = 60\n",
    "epochs_per_quarter = total_epochs // 4\n",
    "\n",
    "eff_batch_size = 112\n",
    "micro_batch    = 16\n",
    "ACCUM_STEPS    = max(1, eff_batch_size // micro_batch)\n",
    "batch_size     = micro_batch\n",
    "\n",
    "lr       = 0.001\n",
    "VAL_FRAC = 0.05\n",
    "\n",
    "N_FOURIER       = 10\n",
    "INCLUDE_RAW_POS = False\n",
    "\n",
    "EPS_RHO   = 1e-3\n",
    "EPS_DET   = 1e-3\n",
    "NEG_DET_W = 1e-2\n",
    "\n",
    "ENABLE_ROI        = False\n",
    "ROI_CONDITIONING  = True\n",
    "K_LOW_MAX         = 0.18\n",
    "K_HIGH_MIN        = 0.22\n",
    "K_TRANS           = 0.04\n",
    "ROI_SIZES         = [40, 48, 56]\n",
    "ROI_TAPER         = 10\n",
    "\n",
    "# Displacement normalization\n",
    "#   \"box\"  -> psi' = psi / BoxSize\n",
    "#   \"cell\" -> psi' = psi / cellsize\n",
    "#   None   -> no normalization\n",
    "DISP_NORM = \"box\"\n",
    "\n",
    "if DISP_NORM == \"box\":\n",
    "    DISP_SCALE = 1.0 / BoxSize\n",
    "elif DISP_NORM == \"cell\":\n",
    "    DISP_SCALE = 1.0 / cellsize\n",
    "else:\n",
    "    DISP_NORM = None\n",
    "    DISP_SCALE = 1.0\n",
    "\n",
    "DISP_INV_SCALE = 1.0 / DISP_SCALE\n",
    "print(f\"[units] DISP_NORM={DISP_NORM}  DISP_SCALE={DISP_SCALE:g}  DISP_INV_SCALE={DISP_INV_SCALE:g}\")\n",
    "\n",
    "WARMUP_EPOCHS  = 5\n",
    "MIN_LR_FACTOR  = 0.08\n",
    "NOCLIP_EPOCHS  = 1\n",
    "MAX_GRAD_NORM  = 5.0\n",
    "\n",
    "PATIENCE = 5\n",
    "\n",
    "DIAG_JSON_EVERY    = 1\n",
    "DIAG_ON_FIRST_LAST = True\n",
    "DIAG_ON_BEST       = True\n",
    "DIAG_TO_STDOUT     = False\n",
    "DIAG_TO_JSONL      = True\n",
    "\n",
    "SEED = 1337\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "generator = torch.Generator(device='cpu')\n",
    "generator.manual_seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1480df-029f-47a7-8138-cf44a5c72475",
   "metadata": {
    "id": "6d1480df-029f-47a7-8138-cf44a5c72475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourier features: Cpos=60 channels\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Precomputed Geometry\n",
    "q_coords = torch.linspace(0, BoxSize - cellsize, grid, device=device)\n",
    "q1, q2, q3 = torch.meshgrid(q_coords, q_coords, q_coords, indexing='ij')\n",
    "Q = torch.stack((q1, q2, q3), dim=0)\n",
    "\n",
    "x_norm = q1 / BoxSize # X coordinate of each point in Q-system \n",
    "y_norm = q2 / BoxSize # Y coordinate of each point in Q-system \n",
    "z_norm = q3 / BoxSize # Z coordinate of each point in Q-system \n",
    "\n",
    "freq_bands = torch.tensor([2.0**k for k in range(N_FOURIER)], device=device)\n",
    "two_pi = 2.0 * np.pi\n",
    "\n",
    "pos_feats = []\n",
    "if INCLUDE_RAW_POS:\n",
    "    pos_feats += [x_norm, y_norm, z_norm]\n",
    "for f in freq_bands:\n",
    "    ax = two_pi * f * x_norm\n",
    "    ay = two_pi * f * y_norm\n",
    "    az = two_pi * f * z_norm\n",
    "    pos_feats += [torch.sin(ax), torch.cos(ax), torch.sin(ay), torch.cos(ay), torch.sin(az), torch.cos(az)]\n",
    "\n",
    "pos_embed = torch.stack(pos_feats, dim=0).contiguous()\n",
    "Cpos = pos_embed.shape[0]\n",
    "print(f\"Fourier features: Cpos={Cpos} channels\")\n",
    "\n",
    "k_base = 2 * np.pi * torch.fft.fftfreq(grid, d=cellsize).to(device)\n",
    "k_z, k_y, k_x = torch.meshgrid(k_base, k_base, k_base, indexing='ij')\n",
    "\n",
    "RAW_DATA_DIR = \"/vast/palmer/pi/padmanabhan/shared/LSS\"\n",
    "data_dir = RAW_DATA_DIR\n",
    "precomputed_dir = PRECOMP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d61b366-41d4-406b-b523-159e3210a887",
   "metadata": {
    "id": "2d61b366-41d4-406b-b523-159e3210a887"
   },
   "outputs": [],
   "source": [
    "# Block 5: Precompute\n",
    "def run_precompute(num_sim: int, data_dir: str, precomputed_dir: str,\n",
    "                   grid: int, BoxSize: float, MAS: str, verbose: bool):\n",
    "    start_time = time.time()\n",
    "    skipped, wrote = 0, 0\n",
    "    for isim in tqdm(range(num_sim), desc=\"Precompute ρ/ρ̄ and ψ̃\"):\n",
    "        pre_file = os.path.join(precomputed_dir, f\"LSS_{isim}_pre.h5\")\n",
    "        if os.path.exists(pre_file):\n",
    "            skipped += 1; continue\n",
    "        file_path = os.path.join(data_dir, f'LSS_{isim}_L100_G64.h5')\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            position     = np.array(f['position'][:],     dtype=np.float32)\n",
    "            displacement = np.array(f['displacement'][:], dtype=np.float32)\n",
    "        rho = np.zeros((grid, grid, grid), dtype=np.float32)\n",
    "        MASL.MA(position, rho, BoxSize, MAS, verbose=verbose)\n",
    "        mean_rho   = np.mean(rho, dtype=np.float64)\n",
    "        rho_rhobar = rho / mean_rho\n",
    "        tilde_psi = -displacement.reshape(grid, grid, grid, 3).astype(np.float32)\n",
    "        os.makedirs(precomputed_dir, exist_ok=True)\n",
    "        with h5py.File(pre_file, 'w') as f:\n",
    "            f.create_dataset('rho_rhobar', data=rho_rhobar)\n",
    "            f.create_dataset('tilde_psi',  data=tilde_psi)\n",
    "        wrote += 1\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Preprocessing took {elapsed:.2f}s | wrote {wrote}, skipped {skipped}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324148fc-3ae3-4b8a-95f1-6f899e9dbbf6",
   "metadata": {
    "id": "324148fc-3ae3-4b8a-95f1-6f899e9dbbf6"
   },
   "outputs": [],
   "source": [
    "# Block 6: Dataset/Loaders\n",
    "class LSSDataset(Dataset):\n",
    "    def __init__(self, precomputed_dir, num_sim, preload=False, cache_size=0, np_dtype=np.float16, scale_disp: float = 1.0):\n",
    "        self.precomputed_dir = precomputed_dir\n",
    "        self.num_sim = int(num_sim)\n",
    "        self.np_dtype = np_dtype\n",
    "        self.cache_size = int(cache_size)\n",
    "        self.scale_disp = float(scale_disp)\n",
    "        self.cache = OrderedDict()\n",
    "        if preload:\n",
    "            for idx in tqdm(range(self.num_sim)):\n",
    "                self.cache[idx] = self._load_np(idx)\n",
    "                if self.cache_size and len(self.cache) > self.cache_size:\n",
    "                    self.cache.popitem(last=False)\n",
    "\n",
    "    def __len__(self): return self.num_sim\n",
    "\n",
    "    def _load_np(self, idx):\n",
    "        pre_file = os.path.join(self.precomputed_dir, f\"LSS_{idx}_pre.h5\")\n",
    "        with h5py.File(pre_file, \"r\") as f:\n",
    "            rho = np.array(f[\"rho_rhobar\"][:], dtype=self.np_dtype)\n",
    "            tilde_psi = np.array(f[\"tilde_psi\"][:], dtype=self.np_dtype)\n",
    "        return rho, tilde_psi\n",
    "\n",
    "    def _to_torch(self, rho_np, psi_np):\n",
    "        rho = torch.from_numpy(rho_np).unsqueeze(0)  # (1, G, G, G)\n",
    "        tilde_psi = torch.from_numpy(psi_np).permute(3, 0, 1, 2) * self.scale_disp  # (3, G, G, G)\n",
    "        return rho, tilde_psi\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            rho_np, psi_np = self.cache.pop(idx); self.cache[idx] = (rho_np, psi_np)\n",
    "            return self._to_torch(rho_np, psi_np)\n",
    "        rho_np, psi_np = self._load_np(idx)\n",
    "        if self.cache_size:\n",
    "            self.cache[idx] = (rho_np, psi_np)\n",
    "            if len(self.cache) > self.cache_size:\n",
    "                self.cache.popitem(last=False)\n",
    "        return self._to_torch(rho_np, psi_np)\n",
    "\n",
    "def make_loaders(precomputed_dir, num_sim, batch_size, VAL_FRAC, device, generator):\n",
    "    dataset = LSSDataset(precomputed_dir, num_sim, preload=False, cache_size=512, np_dtype=np.float32,\n",
    "                         scale_disp=DISP_SCALE)\n",
    "    n_total = len(dataset)\n",
    "    n_val = max(1, int(round(n_total * VAL_FRAC)))\n",
    "    perm = torch.randperm(n_total, generator=generator).tolist()\n",
    "    val_idx = perm[:n_val]; train_idx = perm[n_val:]\n",
    "    train_ds = Subset(dataset, train_idx); val_ds = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True, num_workers=threads,\n",
    "        pin_memory=(device.type == 'cuda'), persistent_workers=True,\n",
    "        worker_init_fn=seed_worker, generator=generator,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False, num_workers=threads,\n",
    "        pin_memory=(device.type == 'cuda'), persistent_workers=True,\n",
    "        worker_init_fn=seed_worker, generator=generator,\n",
    "    )\n",
    "    return dataset, train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6425c4-749b-49bb-92b1-4e7dbdf2341a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af6425c4-749b-49bb-92b1-4e7dbdf2341a",
    "outputId": "15a307a0-6faf-4f27-d637-726b42d5dc9c"
   },
   "outputs": [],
   "source": [
    "# Block 7: Model\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c: int, out_c: int, num_groups: int = 8):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.gn1   = nn.GroupNorm(num_groups=num_groups, num_channels=out_c)\n",
    "        self.act   = nn.SiLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.gn2   = nn.GroupNorm(num_groups=num_groups, num_channels=out_c)\n",
    "        self.skip  = nn.Conv3d(in_c, out_c, kernel_size=1) if in_c != out_c else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.act(self.gn1(self.conv1(x)))\n",
    "        h = self.gn2(self.conv2(h))\n",
    "        return self.act(h + self.skip(x))\n",
    "\n",
    "\n",
    "class ASPP3D(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, rates=(1, 2, 4)):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Conv3d(c_in, c_out, kernel_size=3, padding=r, dilation=r) for r in rates\n",
    "        ])\n",
    "        self.proj = nn.Conv3d(len(rates) * c_out, c_out, kernel_size=1)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        xs = [b(x) for b in self.branches]\n",
    "        return self.act(self.proj(torch.cat(xs, dim=1)))\n",
    "\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, pos_embed: torch.Tensor, use_cond: bool = False):\n",
    "        super().__init__()\n",
    "        self.use_cond = bool(use_cond)\n",
    "        self.register_buffer('pos_embed', pos_embed.float(), persistent=False)\n",
    "        Cpos = self.pos_embed.shape[0]\n",
    "\n",
    "        C1, C2, C3, C4 = 48, 96, 192, 320\n",
    "\n",
    "        self.proj_rho  = nn.Conv3d(1,    C1, kernel_size=3, padding=1)\n",
    "        self.proj_pos  = nn.Conv3d(Cpos, C1, kernel_size=3, padding=1, bias=False)\n",
    "        if self.use_cond:\n",
    "            self.proj_cond = nn.Conv3d(5, C1, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.down1 = ResBlock(C1, C1);  self.pool1 = nn.MaxPool3d(2)\n",
    "        self.down2 = ResBlock(C1, C2);  self.pool2 = nn.MaxPool3d(2)\n",
    "        self.down3 = ResBlock(C2, C3);  self.pool3 = nn.MaxPool3d(2)\n",
    "\n",
    "        self.bottleneck = ASPP3D(C3, C4, rates=(1, 2, 4))\n",
    "\n",
    "        self.up3_pre = nn.Sequential(nn.ConvTranspose3d(C4, C3, kernel_size=2, stride=2), nn.SiLU(inplace=True))\n",
    "        self.conv3   = ResBlock(C3 + C3, C3)\n",
    "\n",
    "        self.up2_pre = nn.Sequential(nn.ConvTranspose3d(C3, C2, kernel_size=2, stride=2), nn.SiLU(inplace=True))\n",
    "        self.conv2   = ResBlock(C2 + C2, C2)\n",
    "\n",
    "        self.up1_pre = nn.Sequential(nn.ConvTranspose3d(C2, C1, kernel_size=2, stride=2), nn.SiLU(inplace=True))\n",
    "        self.conv1   = ResBlock(C1 + C1, C1)\n",
    "\n",
    "        self.final = nn.Conv3d(C1, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        B, _, G, _, _ = x.shape\n",
    "\n",
    "        pos = self.pos_embed.unsqueeze(0).to(dtype=x.dtype)\n",
    "        pos_f = self.proj_pos(pos)\n",
    "\n",
    "        x_f = self.proj_rho(x)\n",
    "\n",
    "        if self.use_cond:\n",
    "            Ccond = self.proj_cond.in_channels\n",
    "            if cond is None:\n",
    "                cond = torch.zeros((B, Ccond, G, G, G), dtype=x.dtype, device=x.device)\n",
    "            else:\n",
    "                if cond.dim() == 5 and cond.shape[0] == 1 and B > 1:\n",
    "                    cond = cond.expand(B, -1, -1, -1, -1).contiguous()\n",
    "                if cond.shape[1] == 1 and Ccond > 1:\n",
    "                    cond = cond.repeat(1, Ccond, 1, 1, 1)\n",
    "                if cond.shape[1] != Ccond:\n",
    "                    if cond.shape[1] > Ccond:\n",
    "                        cond = cond[:, :Ccond]\n",
    "                    else:\n",
    "                        reps = (Ccond + cond.shape[1] - 1) // cond.shape[1]\n",
    "                        cond = cond.repeat(1, reps, 1, 1, 1)[:, :Ccond]\n",
    "            c_f = self.proj_cond(cond)\n",
    "            h0 = x_f + pos_f + c_f\n",
    "        else:\n",
    "            h0 = x_f + pos_f\n",
    "\n",
    "        d1 = self.down1(h0); p1 = self.pool1(d1)\n",
    "        d2 = self.down2(p1); p2 = self.pool2(d2)\n",
    "        d3 = self.down3(p2); p3 = self.pool3(d3)\n",
    "\n",
    "        b  = self.bottleneck(p3)\n",
    "\n",
    "        u3 = self.up3_pre(b); c3 = self.conv3(torch.cat([u3, d3], dim=1))\n",
    "        u2 = self.up2_pre(c3); c2 = self.conv2(torch.cat([u2, d2], dim=1))\n",
    "        u1 = self.up1_pre(c2); c1 = self.conv1(torch.cat([u1, d1], dim=1))\n",
    "\n",
    "        return self.final(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb154aec-6886-479d-a991-47d1b0c14492",
   "metadata": {
    "id": "cb154aec-6886-479d-a991-47d1b0c14492"
   },
   "outputs": [],
   "source": [
    "# Block 8: Spatial Losses\n",
    "def _sobel_kernels_3d(dtype, device):\n",
    "    s = torch.tensor([1.0, 2.0, 1.0], dtype=dtype, device=device)\n",
    "    d = torch.tensor([1.0, 0.0, -1.0], dtype=dtype, device=device)\n",
    "    Gx = torch.einsum('z,y,x->zyx', s, s, d) / 32.0\n",
    "    Gy = torch.einsum('z,y,x->zyx', s, d, s) / 32.0\n",
    "    Gz = torch.einsum('z,y,x->zyx', d, s, s) / 32.0\n",
    "    K = torch.stack([Gx, Gy, Gz], dim=0).unsqueeze(1)\n",
    "    return K\n",
    "\n",
    "def grad_loss_3d(pred, target):\n",
    "    assert pred.shape == target.shape\n",
    "    B, C, D, H, W = pred.shape\n",
    "    K = _sobel_kernels_3d(dtype=pred.dtype, device=pred.device)\n",
    "    loss = 0.0\n",
    "    for axis in range(3):\n",
    "        w_axis = K[axis:axis+1].repeat(C, 1, 1, 1, 1)\n",
    "        gp = F.conv3d(pred,   w_axis, padding=1, groups=C)\n",
    "        gt = F.conv3d(target, w_axis, padding=1, groups=C)\n",
    "        loss = loss + F.l1_loss(gp, gt)\n",
    "    return loss / 3.0\n",
    "\n",
    "def displacement_loss_3d(pred, target, w_data=1.0, w_grad=0.3):\n",
    "    return w_data * F.l1_loss(pred, target) + w_grad * grad_loss_3d(pred, target)\n",
    "\n",
    "def _laplacian_kernel_3d(dtype, device):\n",
    "    K = torch.zeros((1,1,3,3,3), dtype=dtype, device=device)\n",
    "    K[0,0,1,1,1] = -6.0\n",
    "    K[0,0,1,1,0] =  1.0\n",
    "    K[0,0,1,1,2] =  1.0\n",
    "    K[0,0,1,0,1] =  1.0\n",
    "    K[0,0,1,2,1] =  1.0\n",
    "    K[0,0,0,1,1] =  1.0\n",
    "    K[0,0,2,1,1] =  1.0\n",
    "    return K\n",
    "\n",
    "def laplacian_loss(pred, target):\n",
    "    assert pred.shape == target.shape\n",
    "    B, C, D, H, W = pred.shape\n",
    "    K = _laplacian_kernel_3d(dtype=pred.dtype, device=pred.device)\n",
    "    Kc = K.repeat(C, 1, 1, 1, 1)\n",
    "    Lp = F.conv3d(pred,   Kc, padding=1, groups=C)\n",
    "    Lt = F.conv3d(target, Kc, padding=1, groups=C)\n",
    "    return F.l1_loss(Lp, Lt)\n",
    "\n",
    "def edge_aware_l1(pred, target, beta=2.0, eps=1e-12):\n",
    "    assert pred.shape == target.shape\n",
    "    B, C, D, H, W = pred.shape\n",
    "    K = _sobel_kernels_3d(dtype=target.dtype, device=target.device)\n",
    "    Kc = K.repeat(C, 1, 1, 1, 1)\n",
    "    gx = F.conv3d(target, Kc[0::3], padding=1, groups=C)\n",
    "    gy = F.conv3d(target, Kc[1::3], padding=1, groups=C)\n",
    "    gz = F.conv3d(target, Kc[2::3], padding=1, groups=C)\n",
    "    grad_mag = torch.sqrt(gx**2 + gy**2 + gz**2)\n",
    "    w = 1.0 + beta * grad_mag\n",
    "    num = (w * torch.abs(pred - target)).sum()\n",
    "    den = w.sum().clamp_min(eps)\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lU-lY5E8RUc2",
   "metadata": {
    "id": "lU-lY5E8RUc2"
   },
   "outputs": [],
   "source": [
    "# Block 9: Spectral Losses\n",
    "k_mag = torch.sqrt(k_x**2 + k_y**2 + k_z**2)\n",
    "k_max = k_mag.max().clamp_min(1e-8)\n",
    "k_weight = (k_mag / k_max).to(dtype=torch.float32)\n",
    "\n",
    "def _soft_step(x, center, width):\n",
    "    t = (x - center) / max(width, 1e-8)\n",
    "    t = torch.clamp(t, -1.0, 1.0)\n",
    "    return 0.5 * (1.0 + torch.sin(0.5 * np.pi * t))\n",
    "\n",
    "def _highpass_mask(kmin, trans):\n",
    "    return _soft_step(k_mag, kmin, trans)\n",
    "\n",
    "def _lowpass_mask(kmax_, trans):\n",
    "    return 1.0 - _soft_step(k_mag, kmax_, trans)\n",
    "\n",
    "def make_partition_masks(k_low_max, k_high_min, trans=0.04):\n",
    "    m_low  = _lowpass_mask(kmax_=k_low_max,  trans=trans)\n",
    "    m_high = _highpass_mask(kmin=k_high_min, trans=trans)\n",
    "    den = (m_low + m_high).clamp_min(1e-6)\n",
    "    return (m_low / den, m_high / den)\n",
    "\n",
    "def spectral_loss_band(pred, target, mask, p=1.0, relative=True, eps=1e-10, floor_frac=0.05):\n",
    "    B, C, D, H, W = pred.shape\n",
    "    norm = (D * H * W)\n",
    "    Fp = torch.fft.fftn(pred.float(),   dim=(2, 3, 4)) / norm\n",
    "    Ft = torch.fft.fftn(target.float(), dim=(2, 3, 4)) / norm\n",
    "    wk = (k_mag / k_max).pow(p).to(dtype=torch.float32)\n",
    "    w  = (mask * wk).unsqueeze(0).unsqueeze(0)\n",
    "    diff2 = (Fp - Ft).abs().pow(2)\n",
    "    if relative:\n",
    "        num   = (diff2 * w).mean()\n",
    "        base  = (Ft.abs().pow(2) * w).mean()\n",
    "        base_unw = (Ft.abs().pow(2) * mask.unsqueeze(0).unsqueeze(0)).mean()\n",
    "        denom = torch.maximum(base, floor_frac * base_unw).clamp_min(eps)\n",
    "        return num / denom\n",
    "    else:\n",
    "        return (diff2 * w).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "igJWMVxARbCJ",
   "metadata": {
    "id": "igJWMVxARbCJ"
   },
   "outputs": [],
   "source": [
    "# Block 10: Schedules\n",
    "def cos_ramp(frac, start, end, y0, y1):\n",
    "    if end <= start:\n",
    "        return y1\n",
    "    t = (frac - start) / (end - start)\n",
    "    t = max(0.0, min(1.0, t))\n",
    "    return y0 + (y1 - y0) * 0.5 * (1.0 - np.cos(np.pi * t))\n",
    "\n",
    "def schedule_weights(epoch_idx: int, total_epochs: int):\n",
    "    f = (epoch_idx + 1) / float(total_epochs)\n",
    "    Wspec_low  = cos_ramp(f, 0.00, 0.67, y0=0.20, y1=0.05)\n",
    "    Wspec_high = cos_ramp(f, 0.50, 1.00, y0=0.06, y1=0.45)\n",
    "    p_high     = cos_ramp(f, 0.50, 1.00, y0=1.20, y1=3.00)\n",
    "    Wgrad      = cos_ramp(f, 0.50, 1.00, y0=0.60, y1=0.85)\n",
    "    Wlap       = cos_ramp(f, 0.50, 1.00, y0=0.20, y1=0.28)\n",
    "    Wedge      = cos_ramp(f, 0.60, 1.00, y0=0.20, y1=0.25)\n",
    "    focus_prob = cos_ramp(f, 0.40, 1.00, y0=0.10, y1=0.55)\n",
    "    Wphys      = cos_ramp(f, 0.00, 0.33, y0=0.30, y1=1.00)\n",
    "    return Wspec_low, Wspec_high, p_high, Wgrad, Wlap, Wedge, focus_prob, Wphys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "WLAVigAAReLI",
   "metadata": {
    "id": "WLAVigAAReLI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh training.\n"
     ]
    }
   ],
   "source": [
    "# Block 11: Model/Opt/Sched/State\n",
    "import math\n",
    "\n",
    "model = UNet3D(pos_embed=pos_embed, use_cond=ROI_CONDITIONING).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def _lr_lambda(e):\n",
    "    e1 = e + 1\n",
    "    if e1 <= WARMUP_EPOCHS:\n",
    "        return e1 / max(1, WARMUP_EPOCHS)\n",
    "    t = (e1 - WARMUP_EPOCHS) / max(1, total_epochs - WARMUP_EPOCHS)\n",
    "    floor = MIN_LR_FACTOR\n",
    "    return floor + 0.5 * (1.0 - floor) * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=_lr_lambda)\n",
    "\n",
    "USE_BF16 = (device.type == 'cuda') and torch.cuda.is_bf16_supported()\n",
    "from torch import bfloat16 as _bf16, float16 as _fp16\n",
    "AMP_DTYPE = _bf16 if USE_BF16 else _fp16\n",
    "scaler = GradScaler('cuda', enabled=(device.type == 'cuda' and not USE_BF16))\n",
    "\n",
    "data_losses, phys_losses, total_losses = [], [], []\n",
    "val_data_losses, val_phys_losses, val_total_losses = [], [], []\n",
    "base_disp_hist, lap_losses_hist, edge_losses_hist = [], [], []\n",
    "spec_low_hist, spec_high_hist = [], []\n",
    "\n",
    "best_loss = float('inf')\n",
    "no_improve = 0\n",
    "start_epoch = 0\n",
    "last_epoch = 0\n",
    "\n",
    "os.makedirs(os.path.dirname(metrics_log_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(train_log_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "is_resuming = False\n",
    "if RESUME_FROM_CHECKPOINT and os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"[warn] strict load failed ({e}); retrying strict=False\")\n",
    "        _incompat = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        print(f\"[info] missing={len(_incompat.missing_keys)} unexpected={len(_incompat.unexpected_keys)}\")\n",
    "    model.to(device)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        try:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] couldn't load scheduler state: {e}\")\n",
    "    data_losses        = checkpoint.get('data_losses', data_losses)\n",
    "    phys_losses        = checkpoint.get('phys_losses', phys_losses)\n",
    "    total_losses       = checkpoint.get('total_losses', total_losses)\n",
    "    val_data_losses    = checkpoint.get('val_data_losses', val_data_losses)\n",
    "    val_phys_losses    = checkpoint.get('val_phys_losses', val_phys_losses)\n",
    "    val_total_losses   = checkpoint.get('val_total_losses', val_total_losses)\n",
    "    best_loss          = checkpoint.get('best_loss', best_loss)\n",
    "    no_improve         = checkpoint.get('no_improve', no_improve)\n",
    "    start_epoch        = checkpoint.get('epoch', 0)\n",
    "    base_disp_hist     = checkpoint.get('base_disp_hist', base_disp_hist)\n",
    "    lap_losses_hist    = checkpoint.get('lap_losses_hist',  lap_losses_hist)\n",
    "    edge_losses_hist   = checkpoint.get('edge_losses_hist', edge_losses_hist)\n",
    "    spec_low_hist      = checkpoint.get('spec_low_hist',    spec_low_hist)\n",
    "    spec_high_hist     = checkpoint.get('spec_high_hist',   spec_high_hist)\n",
    "    is_resuming = True\n",
    "    print(f\"Resumed from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Starting fresh training.\")\n",
    "    if RESET_LOGS_ON_FRESH_START:\n",
    "        for p in (metrics_log_path, train_log_path, diag_jsonl_path):\n",
    "            with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"# New run: {RUN_NAME} @ {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3qCC5rmYRiZn",
   "metadata": {
    "id": "3qCC5rmYRiZn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:19<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60 | Base: 0.159118  Lap: 0.355874  Edge: 0.137300 | SpecLow: 3.610861  SpecHigh: 313.412206 | Data: 19.785622  Phys: 0.000000  Total: 19.785622 | LR=0.0002 GradNorm=255.391 NegDet=0.000 | Val: Data=4.640830 Phys=0.000000 Total=4.640830 NegDet=0.000 | Sched(Wlow=0.200, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.30) | Time: 20.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/60 | Base: 0.113818  Lap: 0.172062  Edge: 0.102356 | SpecLow: 1.329519  SpecHigh: 50.664782 | Data: 3.474947  Phys: 0.000000  Total: 3.474947 | LR=0.0004 GradNorm=34.685 NegDet=0.000 | Val: Data=2.853955 Phys=0.000000 Total=2.853955 NegDet=0.000 | Sched(Wlow=0.199, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.32) | Time: 19.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/60 | Base: 0.101594  Lap: 0.139764  Edge: 0.091793 | SpecLow: 1.141837  SpecHigh: 33.763657 | Data: 2.401057  Phys: 0.000000  Total: 2.401057 | LR=0.0006 GradNorm=19.546 NegDet=0.000 | Val: Data=1.936450 Phys=0.000000 Total=1.936450 NegDet=0.000 | Sched(Wlow=0.198, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.34) | Time: 19.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/60 | Base: 0.087295  Lap: 0.119567  Edge: 0.079058 | SpecLow: 0.999558  SpecHigh: 21.713973 | Data: 1.627190  Phys: 0.000000  Total: 1.627190 | LR=0.0008 GradNorm=9.390 NegDet=0.000 | Val: Data=1.331469 Phys=0.000000 Total=1.331469 NegDet=0.000 | Sched(Wlow=0.196, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.37) | Time: 19.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/60 | Base: 0.076371  Lap: 0.102202  Edge: 0.069252 | SpecLow: 0.832455  SpecHigh: 14.176904 | Data: 1.123888  Phys: 0.000000  Total: 1.123888 | LR=0.001 GradNorm=5.483 NegDet=0.000 | Val: Data=0.906593 Phys=0.000000 Total=0.906593 NegDet=0.000 | Sched(Wlow=0.194, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.40) | Time: 19.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/60 | Base: 0.059247  Lap: 0.090167  Edge: 0.053049 | SpecLow: 0.719227  SpecHigh: 9.426896 | Data: 0.791913  Phys: 0.000000  Total: 0.791913 | LR=0.00099925 GradNorm=3.222 NegDet=0.000 | Val: Data=0.680030 Phys=0.000000 Total=0.680030 NegDet=0.000 | Sched(Wlow=0.192, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.45) | Time: 19.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/60 | Base: 0.045588  Lap: 0.079829  Edge: 0.039982 | SpecLow: 0.585496  SpecHigh: 6.980217 | Data: 0.599158  Phys: 0.000000  Total: 0.599158 | LR=0.000997 GradNorm=2.390 NegDet=0.000 | Val: Data=0.519326 Phys=0.000000 Total=0.519326 NegDet=0.000 | Sched(Wlow=0.189, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.49) | Time: 19.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/60 | Base: 0.039418  Lap: 0.070352  Edge: 0.034278 | SpecLow: 0.430550  SpecHigh: 5.335244 | Data: 0.460518  Phys: 0.000000  Total: 0.460518 | LR=0.00099326 GradNorm=1.713 NegDet=0.000 | Val: Data=0.398804 Phys=0.000000 Total=0.398804 NegDet=0.000 | Sched(Wlow=0.186, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.55) | Time: 19.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/60 | Base: 0.035734  Lap: 0.064379  Edge: 0.030936 | SpecLow: 0.296150  SpecHigh: 4.211337 | Data: 0.361488  Phys: 0.000000  Total: 0.361488 | LR=0.00098805 GradNorm=1.457 NegDet=0.000 | Val: Data=0.322163 Phys=0.000000 Total=0.322163 NegDet=0.000 | Sched(Wlow=0.182, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.60) | Time: 19.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/60 | Base: 0.034105  Lap: 0.057636  Edge: 0.029550 | SpecLow: 0.254357  SpecHigh: 3.236343 | Data: 0.291115  Phys: 0.000000  Total: 0.291115 | LR=0.00098137 GradNorm=1.167 NegDet=0.000 | Val: Data=0.260446 Phys=0.000000 Total=0.260446 NegDet=0.000 | Sched(Wlow=0.178, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.66) | Time: 19.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/60 | Base: 0.033839  Lap: 0.051318  Edge: 0.029449 | SpecLow: 0.249387  SpecHigh: 2.420519 | Data: 0.238672  Phys: 0.000000  Total: 0.238672 | LR=0.00097325 GradNorm=0.940 NegDet=0.000 | Val: Data=0.214540 Phys=0.000000 Total=0.214540 NegDet=0.000 | Sched(Wlow=0.174, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.71) | Time: 19.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/60 | Base: 0.031626  Lap: 0.046685  Edge: 0.027375 | SpecLow: 0.224430  SpecHigh: 1.888171 | Data: 0.197788  Phys: 0.000000  Total: 0.197788 | LR=0.00096372 GradNorm=0.695 NegDet=0.000 | Val: Data=0.184783 Phys=0.000000 Total=0.184783 NegDet=0.000 | Sched(Wlow=0.169, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.76) | Time: 19.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/60 | Base: 0.029192  Lap: 0.043857  Edge: 0.025076 | SpecLow: 0.203530  SpecHigh: 1.631395 | Data: 0.174370  Phys: 0.000000  Total: 0.174370 | LR=0.0009528 GradNorm=0.505 NegDet=0.000 | Val: Data=0.167200 Phys=0.000000 Total=0.167200 NegDet=0.000 | Sched(Wlow=0.165, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.82) | Time: 19.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/60 | Base: 0.027925  Lap: 0.042040  Edge: 0.023915 | SpecLow: 0.195915  SpecHigh: 1.493423 | Data: 0.161968  Phys: 0.000000  Total: 0.161968 | LR=0.00094054 GradNorm=0.529 NegDet=0.000 | Val: Data=0.157805 Phys=0.000000 Total=0.157805 NegDet=0.000 | Sched(Wlow=0.159, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.86) | Time: 19.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/60 | Base: 0.026775  Lap: 0.040872  Edge: 0.022848 | SpecLow: 0.183796  SpecHigh: 1.403736 | Data: 0.152076  Phys: 0.000000  Total: 0.152076 | LR=0.00092698 GradNorm=0.450 NegDet=0.000 | Val: Data=0.151026 Phys=0.000000 Total=0.151026 NegDet=0.000 | Sched(Wlow=0.154, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.90) | Time: 19.20s\n",
      "Checkpoint saved at epoch 15 (25% complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/60 | Base: 0.026086  Lap: 0.039766  Edge: 0.022213 | SpecLow: 0.174133  SpecHigh: 1.343048 | Data: 0.144950  Phys: 0.000000  Total: 0.144950 | LR=0.00091215 GradNorm=0.383 NegDet=0.000 | Val: Data=0.142289 Phys=0.000000 Total=0.142289 NegDet=0.000 | Sched(Wlow=0.149, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.94) | Time: 19.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/60 | Base: 0.025585  Lap: 0.039004  Edge: 0.021759 | SpecLow: 0.166488  SpecHigh: 1.295050 | Data: 0.139252  Phys: 0.000000  Total: 0.139252 | LR=0.00089611 GradNorm=0.357 NegDet=0.000 | Val: Data=0.137445 Phys=0.000000 Total=0.137445 NegDet=0.000 | Sched(Wlow=0.143, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.97) | Time: 19.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/60 | Base: 0.025602  Lap: 0.038521  Edge: 0.021806 | SpecLow: 0.166821  SpecHigh: 1.262157 | Data: 0.136299  Phys: 0.000000  Total: 0.136299 | LR=0.0008789 GradNorm=0.473 NegDet=0.000 | Val: Data=0.136274 Phys=0.000000 Total=0.136274 NegDet=0.000 | Sched(Wlow=0.137, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=0.99) | Time: 19.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/60 | Base: 0.024804  Lap: 0.038195  Edge: 0.021048 | SpecLow: 0.155272  SpecHigh: 1.235048 | Data: 0.131167  Phys: 0.000000  Total: 0.131167 | LR=0.00086059 GradNorm=0.352 NegDet=0.000 | Val: Data=0.130391 Phys=0.000000 Total=0.130391 NegDet=0.000 | Sched(Wlow=0.131, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=1.00) | Time: 19.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/60 | Base: 0.024310  Lap: 0.037770  Edge: 0.020592 | SpecLow: 0.147395  SpecHigh: 1.206057 | Data: 0.126858  Phys: 0.000000  Total: 0.126858 | LR=0.00084124 GradNorm=0.301 NegDet=0.000 | Val: Data=0.127054 Phys=0.000000 Total=0.127054 NegDet=0.000 | Sched(Wlow=0.126, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=1.00) | Time: 19.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/60 | Base: 0.024180  Lap: 0.037149  Edge: 0.020491 | SpecLow: 0.147662  SpecHigh: 1.179193 | Data: 0.124140  Phys: 0.000000  Total: 0.124140 | LR=0.0008209 GradNorm=0.269 NegDet=0.000 | Val: Data=0.124198 Phys=0.000000 Total=0.124198 NegDet=0.000 | Sched(Wlow=0.120, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=1.00) | Time: 19.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/60 | Base: 0.024011  Lap: 0.036888  Edge: 0.020348 | SpecLow: 0.146954  SpecHigh: 1.161850 | Data: 0.121910  Phys: 0.000000  Total: 0.121910 | LR=0.00079964 GradNorm=0.287 NegDet=0.000 | Val: Data=0.121162 Phys=0.000000 Total=0.121162 NegDet=0.000 | Sched(Wlow=0.114, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=1.00) | Time: 19.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/60 | Base: 0.023786  Lap: 0.036706  Edge: 0.020151 | SpecLow: 0.140382  SpecHigh: 1.145006 | Data: 0.119043  Phys: 0.000000  Total: 0.119043 | LR=0.00077754 GradNorm=0.355 NegDet=0.000 | Val: Data=0.118409 Phys=0.000000 Total=0.118409 NegDet=0.000 | Sched(Wlow=0.108, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=1.00) | Time: 19.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/60 | Base: 0.023270  Lap: 0.036373  Edge: 0.019667 | SpecLow: 0.131934  SpecHigh: 1.125070 | Data: 0.115507  Phys: 0.000000  Total: 0.115507 | LR=0.00075467 GradNorm=0.264 NegDet=0.000 | Val: Data=0.115774 Phys=0.000000 Total=0.115774 NegDet=0.000 | Sched(Wlow=0.102, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=1.00) | Time: 19.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60 | Base: 0.023223  Lap: 0.036173  Edge: 0.019642 | SpecLow: 0.131491  SpecHigh: 1.110091 | Data: 0.113746  Phys: 0.000000  Total: 0.113746 | LR=0.00073109 GradNorm=0.325 NegDet=0.000 | Val: Data=0.113462 Phys=0.000000 Total=0.113462 NegDet=0.000 | Sched(Wlow=0.097, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=1.00) | Time: 19.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/60 | Base: 0.022908  Lap: 0.035959  Edge: 0.019352 | SpecLow: 0.130766  SpecHigh: 1.095736 | Data: 0.111697  Phys: 0.000000  Total: 0.111697 | LR=0.00070689 GradNorm=0.227 NegDet=0.000 | Val: Data=0.112191 Phys=0.000000 Total=0.112191 NegDet=0.000 | Sched(Wlow=0.092, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.10, Wphys=1.00) | Time: 19.31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/60 | Base: 0.022657  Lap: 0.035771  Edge: 0.019127 | SpecLow: 0.124611  SpecHigh: 1.081922 | Data: 0.109331  Phys: 0.000000  Total: 0.109331 | LR=0.00068215 GradNorm=0.261 NegDet=0.000 | Val: Data=0.108993 Phys=0.000000 Total=0.108993 NegDet=0.000 | Sched(Wlow=0.086, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.11, Wphys=1.00) | Time: 19.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/60 | Base: 0.022159  Lap: 0.035599  Edge: 0.018657 | SpecLow: 0.116534  SpecHigh: 1.066597 | Data: 0.106514  Phys: 0.000000  Total: 0.106514 | LR=0.00065694 GradNorm=0.159 NegDet=0.000 | Val: Data=0.107061 Phys=0.000000 Total=0.107061 NegDet=0.000 | Sched(Wlow=0.082, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.11, Wphys=1.00) | Time: 19.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/60 | Base: 0.022068  Lap: 0.035422  Edge: 0.018585 | SpecLow: 0.114748  SpecHigh: 1.055675 | Data: 0.105040  Phys: 0.000000  Total: 0.105040 | LR=0.00063135 GradNorm=0.223 NegDet=0.000 | Val: Data=0.105268 Phys=0.000000 Total=0.105268 NegDet=0.000 | Sched(Wlow=0.077, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.12, Wphys=1.00) | Time: 19.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/60 | Base: 0.021858  Lap: 0.035296  Edge: 0.018397 | SpecLow: 0.111782  SpecHigh: 1.044207 | Data: 0.103365  Phys: 0.000000  Total: 0.103365 | LR=0.00060546 GradNorm=0.221 NegDet=0.000 | Val: Data=0.103723 Phys=0.000000 Total=0.103723 NegDet=0.000 | Sched(Wlow=0.073, Whigh=0.060, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.13, Wphys=1.00) | Time: 19.25s\n",
      "Checkpoint saved at epoch 30 (50% complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/60 | Base: 0.021630  Lap: 0.035180  Edge: 0.018185 | SpecLow: 0.107101  SpecHigh: 1.035856 | Data: 0.102914  Phys: 0.000000  Total: 0.102914 | LR=0.00057936 GradNorm=0.233 NegDet=0.000 | Val: Data=0.103161 Phys=0.000000 Total=0.103161 NegDet=0.000 | Sched(Wlow=0.069, Whigh=0.061, pH=1.20, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.14, Wphys=1.00) | Time: 19.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/60 | Base: 0.021357  Lap: 0.035020  Edge: 0.017924 | SpecLow: 0.105819  SpecHigh: 1.027494 | Data: 0.104871  Phys: 0.000000  Total: 0.104871 | LR=0.00055314 GradNorm=0.180 NegDet=0.000 | Val: Data=0.105263 Phys=0.000000 Total=0.105263 NegDet=0.000 | Sched(Wlow=0.065, Whigh=0.064, pH=1.22, Wg=0.60, Wlap=0.20, Wedge=0.20, ROI=0.15, Wphys=1.00) | Time: 19.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12/60 [00:04<00:14,  3.21it/s]"
     ]
    }
   ],
   "source": [
    "# Block 12: Train/Eval Loop\n",
    "@torch.no_grad()\n",
    "def evaluate_epoch(model, loader, mask_low, mask_high,\n",
    "                   Wgrad, Wlap, Wedge, Wspec_low, Wspec_high, Wphys, p_high):\n",
    "    model.eval()\n",
    "    s_base = s_lap = s_edge = s_slow = s_shigh = s_data = s_phys = s_neg = 0.0\n",
    "    n = 0\n",
    "    for rho, tilde_psi in loader:\n",
    "        rho = rho.to(device, dtype=torch.float32, non_blocking=True)\n",
    "        tilde_psi = tilde_psi.to(device, dtype=torch.float32, non_blocking=True)\n",
    "        pred = model(rho, cond=None)\n",
    "\n",
    "        base  = displacement_loss_3d(pred, tilde_psi, w_data=1.0, w_grad=Wgrad)\n",
    "        lap   = laplacian_loss(pred, tilde_psi)\n",
    "        edge  = edge_aware_l1(pred, tilde_psi, beta=2.0)\n",
    "        slow  = spectral_loss_band(pred, tilde_psi, mask=mask_low,  p=0.25,   relative=True)\n",
    "        shigh = spectral_loss_band(pred, tilde_psi, mask=mask_high, p=p_high, relative=True)\n",
    "\n",
    "        # phys, neg = compute_physics_loss(rho, pred, disp_inv_scale=DISP_INV_SCALE, return_aux=True)\n",
    "        # phys = phys * (DISP_SCALE**2)\n",
    "        \n",
    "        phys = torch.tensor(0.0, device=pred.device)\n",
    "        neg  = torch.tensor(0.0, device=pred.device)\n",
    "\n",
    "        dc = (pred.mean(dim=(2,3,4))**2).mean()\n",
    "        \n",
    "        data = base + Wlap*lap + Wedge*edge + Wspec_low*slow + Wspec_high*shigh + 0.2*dc\n",
    "\n",
    "        s_base += float(base.detach().cpu());   s_lap  += float(lap.detach().cpu())\n",
    "        s_edge += float(edge.detach().cpu());   s_slow += float(slow.detach().cpu())\n",
    "        s_shigh+= float(shigh.detach().cpu());  s_data += float(data.detach().cpu())\n",
    "        # s_phys += float(phys.detach().cpu());   s_neg  += float(neg.detach().cpu())\n",
    "        \n",
    "        s_phys += 0.0;  s_neg += 0.0\n",
    "        \n",
    "        n += 1\n",
    "    if n == 0:\n",
    "        return (0.0,)*9\n",
    "    avg_base = s_base/n; avg_lap = s_lap/n; avg_edge = s_edge/n\n",
    "    avg_slow = s_slow/n; avg_shigh = s_shigh/n\n",
    "    avg_data = s_data/n; avg_phys = s_phys/n; avg_neg = s_neg/n\n",
    "    # avg_total = avg_data + (lambda_p * Wphys) * avg_phys\n",
    "    \n",
    "    avg_total = avg_data  # 不再加物理项\n",
    "    \n",
    "    return avg_base, avg_lap, avg_edge, avg_slow, avg_shigh, avg_data, avg_phys, avg_total, avg_neg\n",
    "\n",
    "start_time = time.time()\n",
    "first_pre = os.path.join(precomputed_dir, \"LSS_0_pre.h5\")\n",
    "if not os.path.exists(first_pre):\n",
    "    run_precompute(num_sim, data_dir, precomputed_dir, grid, BoxSize, MAS, verbose)\n",
    "\n",
    "dataset, train_loader, val_loader = make_loaders(precomputed_dir, num_sim, batch_size, VAL_FRAC, device, generator)\n",
    "\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    total_base = total_lap = total_edge = 0.0\n",
    "    total_spec_low = total_spec_high = 0.0\n",
    "    total_data = total_phys = 0.0\n",
    "    neg_det_accum = 0.0\n",
    "    grad_norm_accum = 0.0\n",
    "    grad_norm_count = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    (W_SPEC_LOW_E, W_SPEC_HIGH_E, P_HIGH_E,\n",
    "     W_GRAD_E,     W_LAPLACE_E,   W_EDGE_E,\n",
    "     FOCUS_PROB_E, W_PHYS_E) = schedule_weights(epoch, total_epochs)\n",
    "\n",
    "    mask_low_e, mask_high_e = make_partition_masks(K_LOW_MAX, K_HIGH_MIN, K_TRANS)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    step_in_accum = 0\n",
    "\n",
    "    for step, (rho, tilde_psi) in enumerate(tqdm(train_loader)):\n",
    "        rho = rho.to(device, dtype=torch.float32, non_blocking=True)\n",
    "        tilde_psi = tilde_psi.to(device, dtype=torch.float32, non_blocking=True)\n",
    "        M = None\n",
    "        cond = None\n",
    "        with autocast('cuda', dtype=AMP_DTYPE, enabled=(device.type == 'cuda')):\n",
    "            pred = model(rho, cond=cond)\n",
    "            if M is None:\n",
    "                base_disp_loss = displacement_loss_3d(pred, tilde_psi, w_data=1.0, w_grad=W_GRAD_E)\n",
    "                lap_loss       = laplacian_loss(pred, tilde_psi)\n",
    "                edge_loss      = edge_aware_l1(pred, tilde_psi, beta=2.0)\n",
    "            else:\n",
    "                base_disp_loss = displacement_loss_3d_masked(pred, tilde_psi, M, w_data=1.0, w_grad=W_GRAD_E)\n",
    "                lap_loss       = laplacian_loss_masked(pred, tilde_psi, M)\n",
    "                edge_loss      = edge_aware_l1_masked(pred, tilde_psi, M, beta=2.0)\n",
    "\n",
    "            spec_low_loss  = spectral_loss_band(pred, tilde_psi, mask=mask_low_e,  p=0.25,     relative=True)\n",
    "            spec_high_loss = spectral_loss_band(pred, tilde_psi, mask=mask_high_e, p=P_HIGH_E, relative=True)\n",
    "            physics_loss  = torch.tensor(0.0, device=pred.device, dtype=pred.dtype)\n",
    "            neg_det_frac  = torch.tensor(0.0, device=pred.device, dtype=pred.dtype)\n",
    "            dc_loss = (pred.mean(dim=(2,3,4))**2).mean()\n",
    "            data_loss = (base_disp_loss + W_LAPLACE_E*lap_loss + W_EDGE_E*edge_loss\n",
    "                         + W_SPEC_LOW_E*spec_low_loss + W_SPEC_HIGH_E*spec_high_loss\n",
    "                         + 0.2*dc_loss)\n",
    "            loss = data_loss  # [PHYS OFF]\n",
    "\n",
    "        loss = loss / ACCUM_STEPS\n",
    "\n",
    "        if scaler.is_enabled():\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        step_in_accum += 1\n",
    "\n",
    "        total_base      += float(base_disp_loss.detach().cpu())\n",
    "        total_lap       += float(lap_loss.detach().cpu())\n",
    "        total_edge      += float(edge_loss.detach().cpu())\n",
    "        total_spec_low  += float(spec_low_loss.detach().cpu())\n",
    "        total_spec_high += float(spec_high_loss.detach().cpu())\n",
    "        total_data      += float(data_loss.detach().cpu())\n",
    "        total_phys      += float(physics_loss.detach().cpu())\n",
    "        neg_det_accum   += float(neg_det_frac.detach().cpu())\n",
    "        n_batches       += 1\n",
    "\n",
    "        if step_in_accum % ACCUM_STEPS == 0:\n",
    "            if scaler.is_enabled():\n",
    "                scaler.unscale_(optimizer)\n",
    "            this_max = float('inf') if (epoch + 1) <= NOCLIP_EPOCHS else MAX_GRAD_NORM\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=this_max)\n",
    "            if scaler.is_enabled():\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            step_in_accum = 0\n",
    "            grad_norm_accum += float(grad_norm if isinstance(grad_norm, float) else grad_norm.detach().cpu())\n",
    "            grad_norm_count += 1\n",
    "\n",
    "    if step_in_accum != 0:\n",
    "        if scaler.is_enabled():\n",
    "            scaler.unscale_(optimizer)\n",
    "        this_max = float('inf') if (epoch + 1) <= NOCLIP_EPOCHS else MAX_GRAD_NORM\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=this_max)\n",
    "        if scaler.is_enabled():\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        grad_norm_accum += float(grad_norm if isinstance(grad_norm, float) else grad_norm.detach().cpu())\n",
    "        grad_norm_count += 1\n",
    "\n",
    "    if n_batches == 0:\n",
    "        continue\n",
    "\n",
    "    avg_base      = total_base      / n_batches\n",
    "    avg_lap       = total_lap       / n_batches\n",
    "    avg_edge      = total_edge      / n_batches\n",
    "    avg_spec_low  = total_spec_low  / n_batches\n",
    "    avg_spec_high = total_spec_high / n_batches\n",
    "    avg_data      = total_data      / n_batches\n",
    "\n",
    "    avg_grad_norm = grad_norm_accum / max(1, grad_norm_count)\n",
    "\n",
    "    avg_phys      = 0.0                   # [PHYS OFF]\n",
    "    avg_total     = avg_data              # [PHYS OFF]\n",
    "    avg_neg_det   = 0.0                   # [PHYS OFF]\n",
    "\n",
    "    (val_base, val_lap, val_edge, val_slow, val_shigh,\n",
    "     val_data, val_phys, val_total, val_neg) = evaluate_epoch(\n",
    "        model, val_loader, mask_low_e, mask_high_e,\n",
    "        W_GRAD_E, W_LAPLACE_E, W_EDGE_E, W_SPEC_LOW_E, W_SPEC_HIGH_E, W_PHYS_E, P_HIGH_E\n",
    "    )\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    line_str = (\n",
    "        f\"Epoch {epoch + 1}/{total_epochs} | \"\n",
    "        f\"Base: {avg_base:.6f}  Lap: {avg_lap:.6f}  Edge: {avg_edge:.6f} | \"\n",
    "        f\"SpecLow: {avg_spec_low:.6f}  SpecHigh: {avg_spec_high:.6f} | \"\n",
    "        f\"Data: {avg_data:.6f}  Phys: {avg_phys:.6f}  Total: {avg_total:.6f} | \"\n",
    "        f\"LR={current_lr:.5g} GradNorm={avg_grad_norm:.3f} NegDet={avg_neg_det:.003f} | \"\n",
    "        f\"Val: Data={val_data:.6f} Phys={val_phys:.6f} Total={val_total:.6f} NegDet={val_neg:.003f} | \"\n",
    "        f\"Sched(Wlow={W_SPEC_LOW_E:.3f}, Whigh={W_SPEC_HIGH_E:.3f}, pH={P_HIGH_E:.2f}, \"\n",
    "        f\"Wg={W_GRAD_E:.2f}, Wlap={W_LAPLACE_E:.2f}, Wedge={W_EDGE_E:.2f}, ROI={FOCUS_PROB_E:.2f}, \"\n",
    "        f\"Wphys={W_PHYS_E:.2f}) | Time: {time.time() - epoch_start:.2f}s\"\n",
    "    )\n",
    "    print(line_str); _append_train_line(line_str)\n",
    "\n",
    "    data_losses.append(avg_data); phys_losses.append(avg_phys); total_losses.append(avg_total)\n",
    "    val_data_losses.append(val_data); val_phys_losses.append(val_phys); val_total_losses.append(val_total)\n",
    "    base_disp_hist.append(avg_base); lap_losses_hist.append(avg_lap); edge_losses_hist.append(avg_edge)\n",
    "    spec_low_hist.append(avg_spec_low); spec_high_hist.append(avg_spec_high)\n",
    "\n",
    "    monitor = val_total\n",
    "    is_best = (monitor < best_loss)\n",
    "\n",
    "    if monitor < best_loss:\n",
    "        best_loss = monitor\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        last_epoch = epoch + 1\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % epochs_per_quarter == 0 or (epoch + 1) == total_epochs:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"data_losses\": data_losses,\n",
    "            \"phys_losses\": phys_losses,\n",
    "            \"total_losses\": total_losses,\n",
    "            \"val_data_losses\": val_data_losses,\n",
    "            \"val_phys_losses\": val_phys_losses,\n",
    "            \"val_total_losses\": val_total_losses,\n",
    "            \"base_disp_hist\": base_disp_hist,\n",
    "            \"lap_losses_hist\": lap_losses_hist,\n",
    "            \"edge_losses_hist\": edge_losses_hist,\n",
    "            \"spec_low_hist\": spec_low_hist,\n",
    "            \"spec_high_hist\": spec_high_hist,\n",
    "            \"best_loss\": best_loss,\n",
    "            \"no_improve\": no_improve,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1} ({(epoch + 1) / total_epochs * 100:.0f}% complete)\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bghal91wRmnn",
   "metadata": {
    "id": "bghal91wRmnn"
   },
   "outputs": [],
   "source": [
    "# Block 13: Finalize\n",
    "if last_epoch == 0:\n",
    "    last_epoch = total_epochs\n",
    "\n",
    "checkpoint = {\n",
    "    \"epoch\": last_epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "    \"data_losses\": data_losses,\n",
    "    \"phys_losses\": phys_losses,\n",
    "    \"total_losses\": total_losses,\n",
    "    \"val_data_losses\": val_data_losses,\n",
    "    \"val_phys_losses\": val_phys_losses,\n",
    "    \"val_total_losses\": val_total_losses,\n",
    "    \"base_disp_hist\": base_disp_hist, \n",
    "    \"lap_losses_hist\": lap_losses_hist,\n",
    "    \"edge_losses_hist\": edge_losses_hist,\n",
    "    \"spec_low_hist\": spec_low_hist,\n",
    "    \"spec_high_hist\": spec_high_hist,\n",
    "    \"best_loss\": best_loss,\n",
    "    \"no_improve\": no_improve,\n",
    "}\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Final checkpoint saved at epoch {last_epoch}\")\n",
    "\n",
    "with open(metrics_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"epoch,train_data,train_phys,train_total,val_data,val_phys,val_total\\n\")\n",
    "    for i in range(len(data_losses)):\n",
    "        vd = val_data_losses[i] if i < len(val_data_losses) else \"\"\n",
    "        vp = val_phys_losses[i] if i < len(val_phys_losses) else \"\"\n",
    "        vt = val_total_losses[i] if i < len(val_total_losses) else \"\"\n",
    "        f.write(f\"{i+1},{data_losses[i]},{phys_losses[i]},{total_losses[i]},{vd},{vp},{vt}\\n\")\n",
    "\n",
    "if last_epoch >= total_epochs and os.path.exists(checkpoint_path):\n",
    "    os.remove(checkpoint_path)\n",
    "    print(f\"Checkpoint deleted as all {total_epochs} epochs completed.\")\n",
    "else:\n",
    "    print(f\"Training stopped at epoch {last_epoch}. Checkpoint retained for resumption.\")\n",
    "\n",
    "print(f\"Training completed. Total time: {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1AMoI_1kRwS3",
   "metadata": {
    "id": "1AMoI_1kRwS3"
   },
   "outputs": [],
   "source": [
    "# Block 14: Viz\n",
    "def cross_correlation_function(field_a, field_b, boxsize=100.0, nbins=50, subtract_mean=True):\n",
    "    assert field_a.shape == field_b.shape\n",
    "    N = field_a.shape[0]\n",
    "    a = field_a.astype(np.float32, copy=False); b = field_b.astype(np.float32, copy=False)\n",
    "    if subtract_mean:\n",
    "        a = a - np.mean(a, dtype=np.float64); b = b - np.mean(b, dtype=np.float64)\n",
    "    Fa = np.fft.fftn(a); Fb = np.fft.fftn(b); V = boxsize ** 3\n",
    "    Pab = Fa * np.conj(Fb) / V\n",
    "    xi = np.fft.ifftn(Pab).real\n",
    "    coords = np.fft.fftfreq(N) * N\n",
    "    z, y, x = np.meshgrid(coords, coords, coords, indexing=\"ij\")\n",
    "    r_grid = np.sqrt(x**2 + y**2 + z**2) * (boxsize / N)\n",
    "    r_flat = r_grid.reshape(-1); xi_flat = xi.reshape(-1)\n",
    "    rmax = np.max(r_flat); bins = np.linspace(0.0, rmax, nbins + 1)\n",
    "    which = np.digitize(r_flat, bins) - 1; nbins_eff = len(bins) - 1\n",
    "    xi_sum = np.zeros(nbins_eff, dtype=np.float64)\n",
    "    r_sum  = np.zeros(nbins_eff, dtype=np.float64)\n",
    "    cnt    = np.zeros(nbins_eff, dtype=np.int64)\n",
    "    for i in range(r_flat.size):\n",
    "        bi = which[i]\n",
    "        if 0 <= bi < nbins_eff:\n",
    "            xi_sum[bi] += xi_flat[i]; r_sum[bi] += r_flat[i]; cnt[bi] += 1\n",
    "    mask  = cnt > 0\n",
    "    r_bin = r_sum[mask] / cnt[mask]; xi_r = xi_sum[mask] / cnt[mask]\n",
    "    return r_bin, xi_r\n",
    "\n",
    "def _make_k_grid(N, boxsize):\n",
    "    kf = 2.0 * np.pi / boxsize\n",
    "    freqs = np.fft.fftfreq(N) * N\n",
    "    kz, ky, kx = np.meshgrid(freqs, freqs, freqs, indexing=\"ij\")\n",
    "    kx = kx * kf; ky = ky * kf; kz = kz * kf\n",
    "    kk = np.sqrt(kx**2 + ky**2 + kz**2)\n",
    "    return kx, ky, kz, kk\n",
    "\n",
    "def power_spectrum_auto(field, boxsize=100.0, nbins=30, subtract_mean=True):\n",
    "    N = field.shape[0]\n",
    "    a = field.astype(np.float32, copy=False)\n",
    "    if subtract_mean: a = a - np.mean(a, dtype=np.float64)\n",
    "    Fa = np.fft.fftn(a); V = boxsize ** 3\n",
    "    P = (Fa * np.conj(Fa)).real / V\n",
    "    _, _, _, kk = _make_k_grid(N, boxsize)\n",
    "    k_flat = kk.reshape(-1); P_flat = P.reshape(-1)\n",
    "    kmax = k_flat.max(); bins = np.linspace(0.0, kmax, nbins + 1)\n",
    "    which = np.digitize(k_flat, bins) - 1; nbins_eff = len(bins) - 1\n",
    "    P_sum = np.zeros(nbins_eff, dtype=np.float64)\n",
    "    k_sum = np.zeros(nbins_eff, dtype=np.float64)\n",
    "    cnt   = np.zeros(nbins_eff, dtype=np.int64)\n",
    "    for i in range(k_flat.size):\n",
    "        bi = which[i]\n",
    "        if 0 <= bi < nbins_eff:\n",
    "            P_sum[bi] += P_flat[i]; k_sum[bi] += k_flat[i]; cnt[bi] += 1\n",
    "    mask = cnt > 0\n",
    "    k_bin = k_sum[mask] / cnt[mask]; Pk = P_sum[mask] / cnt[mask]\n",
    "    return k_bin, Pk\n",
    "\n",
    "def power_spectrum_cross(field_a, field_b, boxsize=100.0, nbins=30, subtract_mean=True):\n",
    "    assert field_a.shape == field_b.shape\n",
    "    N = field_a.shape[0]\n",
    "    a = field_a.astype(np.float32, copy=False); b = field_b.astype(np.float32, copy=False)\n",
    "    if subtract_mean:\n",
    "        a = a - np.mean(a, dtype=np.float64); b = b - np.mean(b, dtype=np.float64)\n",
    "    Fa = np.fft.fftn(a); Fb = np.fft.fftn(b); V = boxsize ** 3\n",
    "    Pab = (Fa * np.conj(Fb)) / V\n",
    "    _, _, _, kk = _make_k_grid(N, boxsize)\n",
    "    k_flat = kk.reshape(-1); P_flat = Pab.real.reshape(-1)\n",
    "    kmax = k_flat.max(); bins = np.linspace(0.0, kmax, nbins + 1)\n",
    "    which = np.digitize(k_flat, bins) - 1; nbins_eff = len(bins) - 1\n",
    "    P_sum = np.zeros(nbins_eff, dtype=np.float64)\n",
    "    k_sum = np.zeros(nbins_eff, dtype=np.float64)\n",
    "    cnt   = np.zeros(nbins_eff, dtype=np.int64)\n",
    "    for i in range(k_flat.size):\n",
    "        bi = which[i]\n",
    "        if 0 <= bi < nbins_eff:\n",
    "            P_sum[bi] += P_flat[i]; k_sum[bi] += k_flat[i]; cnt[bi] += 1\n",
    "    mask = cnt > 0\n",
    "    k_bin = k_sum[mask] / cnt[mask]; Pk = P_sum[mask] / cnt[mask]\n",
    "    return k_bin, Pk\n",
    "\n",
    "if 'data_losses' not in globals() or not isinstance(data_losses, list) or len(data_losses) == 0:\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "        data_losses       = ckpt.get('data_losses', [])\n",
    "        phys_losses       = ckpt.get('phys_losses', [])\n",
    "        total_losses      = ckpt.get('total_losses', [])\n",
    "        base_disp_hist    = ckpt.get('base_disp_hist', [])\n",
    "        lap_losses_hist   = ckpt.get('lap_losses_hist', [])\n",
    "        edge_losses_hist  = ckpt.get('edge_losses_hist', [])\n",
    "        spec_low_hist     = ckpt.get('spec_low_hist', [])\n",
    "        spec_high_hist    = ckpt.get('spec_high_hist', [])\n",
    "        val_data_losses   = ckpt.get('val_data_losses', [])\n",
    "        val_phys_losses   = ckpt.get('val_phys_losses', [])\n",
    "        val_total_losses  = ckpt.get('val_total_losses', [])\n",
    "    else:\n",
    "        data_losses = phys_losses = total_losses = []\n",
    "        base_disp_hist = lap_losses_hist = edge_losses_hist = []\n",
    "        spec_low_hist = spec_high_hist = []\n",
    "        val_data_losses = val_phys_losses = val_total_losses = []\n",
    "\n",
    "_EPS = 1e-12\n",
    "def _safe_pos(seq):\n",
    "    arr = np.array(seq, dtype=np.float64) if len(seq) else np.array([], dtype=np.float64)\n",
    "    if arr.size: arr = np.maximum(arr, _EPS)\n",
    "    return arr\n",
    "\n",
    "epochs_list = list(range(1, len(data_losses) + 1))\n",
    "plt.figure(figsize=(10, 6))\n",
    "if len(data_losses): plt.semilogy(epochs_list, _safe_pos(data_losses), label='Train Data')\n",
    "if len(phys_losses): plt.semilogy(epochs_list, _safe_pos(phys_losses), label='Train Phys')\n",
    "if len(total_losses): plt.semilogy(epochs_list, _safe_pos(total_losses), label='Train Total')\n",
    "if len(val_total_losses):\n",
    "    ev = list(range(1, len(val_total_losses) + 1))\n",
    "    plt.semilogy(ev, _safe_pos(val_total_losses), label='Val Total', linestyle='--')\n",
    "if len(val_data_losses):\n",
    "    evd = list(range(1, len(val_data_losses) + 1))\n",
    "    plt.semilogy(evd, _safe_pos(val_data_losses), label='Val Data', linestyle='--')\n",
    "if len(val_phys_losses):\n",
    "    evp = list(range(1, len(val_phys_losses) + 1))\n",
    "    plt.semilogy(evp, _safe_pos(val_phys_losses), label='Val Phys', linestyle='--')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss (log)'); plt.title('Losses'); plt.legend(); plt.grid(True, which='both', ls=':'); plt.tight_layout(); plt.show()\n",
    "\n",
    "have_terms = any(len(x) for x in [base_disp_hist, lap_losses_hist, edge_losses_hist, spec_low_hist, spec_high_hist])\n",
    "if have_terms:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if len(base_disp_hist):  plt.semilogy(range(1, len(base_disp_hist)+1),  _safe_pos(base_disp_hist),  label='Base')\n",
    "    if len(lap_losses_hist): plt.semilogy(range(1, len(lap_losses_hist)+1), _safe_pos(lap_losses_hist), label='Lap')\n",
    "    if len(edge_losses_hist):plt.semilogy(range(1, len(edge_losses_hist)+1),_safe_pos(edge_losses_hist),label='Edge')\n",
    "    if len(spec_low_hist):   plt.semilogy(range(1, len(spec_low_hist)+1),   _safe_pos(spec_low_hist),   label='SpecLow')\n",
    "    if len(spec_high_hist):  plt.semilogy(range(1, len(spec_high_hist)+1),  _safe_pos(spec_high_hist),  label='SpecHigh')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Term (log)'); plt.title('Train Terms'); plt.legend(); plt.grid(True, which='both', ls=':'); plt.tight_layout(); plt.show()\n",
    "\n",
    "if 'model' not in globals():\n",
    "    try:\n",
    "        model = UNet3D(pos_embed=pos_embed, use_cond=ROI_CONDITIONING).to(device)\n",
    "        if os.path.exists(best_model_path):\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "            print(f\"Loaded best model: {best_model_path}\")\n",
    "        elif os.path.exists(checkpoint_path):\n",
    "            ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(ckpt['model_state_dict'])\n",
    "            print(f\"Loaded model from checkpoint: {checkpoint_path}\")\n",
    "        else:\n",
    "            print(\"No model weights found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Model build failed: {e}\")\n",
    "\n",
    "if 'dataset' not in globals():\n",
    "    dataset = LSSDataset(precomputed_dir, num_sim, preload=False, cache_size=64, np_dtype=np.float16,\n",
    "                         scale_disp=DISP_SCALE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        rho_test, tilde_real = dataset[0]\n",
    "    except Exception:\n",
    "        ridx = np.random.randint(0, len(dataset))\n",
    "        rho_test, tilde_real = dataset[ridx]\n",
    "    rho_test = rho_test.unsqueeze(0).to(device, dtype=torch.float32)\n",
    "    pred = model(rho_test)[0].cpu()\n",
    "    tilde_real = tilde_real.cpu()\n",
    "\n",
    "pred_np = pred.numpy().astype(np.float32)\n",
    "real_np = tilde_real.numpy().astype(np.float32)\n",
    "\n",
    "xi_cross_components = []; r_vals = None\n",
    "for c in range(3):\n",
    "    r, xi = cross_correlation_function(real_np[c], pred_np[c], boxsize=BoxSize, nbins=50, subtract_mean=True)\n",
    "    xi_cross_components.append(xi)\n",
    "    if r_vals is None: r_vals = r\n",
    "xi_cross_avg = np.mean(np.stack(xi_cross_components, axis=0), axis=0)\n",
    "\n",
    "xi_true_components = []\n",
    "for c in range(3):\n",
    "    _, xi_t = cross_correlation_function(real_np[c], real_np[c], boxsize=BoxSize, nbins=50, subtract_mean=True)\n",
    "    xi_true_components.append(xi_t)\n",
    "xi_true_avg = np.mean(np.stack(xi_true_components, axis=0), axis=0)\n",
    "\n",
    "xi_pred_components = []\n",
    "for c in range(3):\n",
    "    _, xi_p = cross_correlation_function(pred_np[c], pred_np[c], boxsize=BoxSize, nbins=50, subtract_mean=True)\n",
    "    xi_pred_components.append(xi_p)\n",
    "xi_pred_avg = np.mean(np.stack(xi_pred_components, axis=0), axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for c, name in enumerate(['x', 'y', 'z']):\n",
    "    plt.plot(r_vals, xi_cross_components[c], label=fr'xi_{name}(r) cross')\n",
    "plt.plot(r_vals, xi_cross_avg, linestyle='--', linewidth=2.0, label='xi_cross avg')\n",
    "plt.plot(r_vals, xi_true_avg, color='k', linewidth=2.0, label='xi_true avg')\n",
    "plt.plot(r_vals, xi_pred_avg, linewidth=2.0, label='xi_pred avg')\n",
    "plt.xlabel('r'); plt.ylabel('xi(r)'); plt.title('Correlations'); plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "NBINS_K = 30\n",
    "Pk_true_components = []; Pk_pred_components = []; Pk_cross_components = []; k_vals = None\n",
    "for c in range(3):\n",
    "    k_t, Pt = power_spectrum_auto(real_np[c], boxsize=BoxSize, nbins=NBINS_K, subtract_mean=True)\n",
    "    k_p, Pp = power_spectrum_auto(pred_np[c],  boxsize=BoxSize, nbins=NBINS_K, subtract_mean=True)\n",
    "    k_c, Pc = power_spectrum_cross(real_np[c], pred_np[c], boxsize=BoxSize, nbins=NBINS_K, subtract_mean=True)\n",
    "    Pk_true_components.append(Pt); Pk_pred_components.append(Pp); Pk_cross_components.append(Pc)\n",
    "    if k_vals is None: k_vals = k_t\n",
    "Pk_true_avg  = np.mean(np.stack(Pk_true_components, axis=0), axis=0)\n",
    "Pk_pred_avg  = np.mean(np.stack(Pk_pred_components, axis=0), axis=0)\n",
    "Pk_cross_avg = np.mean(np.stack(Pk_cross_components, axis=0), axis=0)\n",
    "eps = 1e-20\n",
    "rk = Pk_cross_avg / np.sqrt((Pk_true_avg + eps) * (Pk_pred_avg + eps))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(k_vals, Pk_true_avg, label='P_true avg', linewidth=2.0)\n",
    "plt.loglog(k_vals, Pk_pred_avg, label='P_pred avg', linewidth=2.0)\n",
    "plt.xlabel('k'); plt.ylabel('P(k)'); plt.title('Auto Power'); plt.legend(); plt.grid(True, which='both', ls=':'); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(k_vals, rk, linewidth=2.0)\n",
    "plt.ylim(0.0, 1.05)\n",
    "plt.xlabel('k'); plt.ylabel('r(k)'); plt.title('Fourier-space correlation coefficient r(k)')\n",
    "plt.grid(True, which='both', ls=':'); plt.tight_layout(); plt.show()\n",
    "\n",
    "component = 0\n",
    "slice_idx = grid // 2\n",
    "real_slice = real_np[component, :, :, slice_idx]\n",
    "pred_slice = pred_np[component, :, :, slice_idx]\n",
    "diff_slice = real_slice - pred_slice\n",
    "v = float(np.max(np.abs([real_slice, pred_slice, diff_slice]))); v = max(v, 1e-6)\n",
    "vmin, vmax = -v, v\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), constrained_layout=True)\n",
    "im1 = axs[0].imshow(real_slice, cmap='RdBu', origin='lower', vmin=vmin, vmax=vmax); axs[0].set_title('Real'); plt.colorbar(im1, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "im2 = axs[1].imshow(pred_slice, cmap='RdBu', origin='lower', vmin=vmin, vmax=vmax); axs[1].set_title('Pred'); plt.colorbar(im2, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "im3 = axs[2].imshow(diff_slice, cmap='RdBu', origin='lower', vmin=vmin, vmax=vmax); axs[2].set_title('Diff'); plt.colorbar(im3, ax=axs[2], fraction=0.046, pad=0.04)\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "print(f\"[slice] comp={component} z={slice_idx} real[min,max]=({real_slice.min():.3g},{real_slice.max():.3g}) \"\n",
    "      f\"pred[min,max]=({pred_slice.min():.3g},{pred_slice.max():.3g}) \"\n",
    "      f\"diff[min,max]=({diff_slice.min():.3g},{diff_slice.max():.3g}) \"\n",
    "      f\"scale=({vmin:.3g},{vmax:.3g})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6722190-0a1d-487e-9bb3-2b293196dddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "My Virtual Environment",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
